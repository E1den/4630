{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/E1den/4630/blob/master/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qApe31Dc6sme",
        "colab_type": "text"
      },
      "source": [
        "##0. General concepts "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUh8rwqn7Ipi",
        "colab_type": "text"
      },
      "source": [
        "AI is the use of computers to simulate cognitive functions. Machine learning is a subset of AI that lets computers enhance their algorithms and simulate learning. Deep learning is a a form of machine learning that had many more layers to its algorithms and can identify what it needs with out it being explicitly programmed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2WcQNTZ61Y5",
        "colab_type": "text"
      },
      "source": [
        "##1. Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekUye69o7EMd",
        "colab_type": "text"
      },
      "source": [
        "When you build a model, you specify an order of layers of specific sizes. Those layers serve different purposes and in turn pass different types of data to the next layer. For example, a convolutional neural network consists of convolutional layers, pooling layers, then finally flattening and fully connected dense layers. The convolution takes in an image of a given size, the pooling layer reduces the size down, etc. The data is finally flattened into an array and passed to dense layers to get the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQP5ryfmn-6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_05ewKO643w",
        "colab_type": "text"
      },
      "source": [
        "##2. Comping a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9irSDofS7JTl",
        "colab_type": "text"
      },
      "source": [
        "The learning rate is the rate at which the model changes due to errors. If the learning rate is high the model will have a larger adjustment if it has an incorrect output. If it has a lower learning rate, it will have a much smaller adjustment. Generally the learning rate deacys the more you train the model. Optimizers are functions that attempt to minimize the loss of a model. They modify the weights of the connections between the nodes of different layers. How they change those weights depends on the optimizer function you choose. For example, stochastic gradient descent finds the direction of the gradient of the loss for a set of weights and modifies them so that the gradient is lower, minimizing loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0ePqrtSomB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZHJ_EDH67zd",
        "colab_type": "text"
      },
      "source": [
        "##3. Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CZdeCAw7J1-",
        "colab_type": "text"
      },
      "source": [
        "If a model has too many nodes in it, it can effecitvely memorize a set of data. This is good for the training datas acurracy but not for the accuracy of any other data tested. For example, a model could have great accuracy in the training data but terrible accuracy for the test data, this is overfitting. If the model, however, is underfit, it doesnt have enough layers/nodes to learn anything from the training data. It will consistently have terrible accuracy. When training you need to provide the data, the prosepective ouputs, and the number of times to run through the training set. The validation data is a testing set to test the model with data it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtFJX-pqoz7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fitted_model = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EtgdY767GgQ",
        "colab_type": "text"
      },
      "source": [
        "##4.  Finetuning  a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uXUVgTo7KWK",
        "colab_type": "text"
      },
      "source": [
        "Finetuning allows you to modify part of a previously trained model. You can effectively freeze a previously created model, and replace or add layers to it. This can greatly speed up your training time because you dont need to wait for all the layers to train, just the new ones. Once you have trained the model with the frozen layer, you can unfreeze some of it to try to get the layers to gel together better and provide a better/more accurate output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwp92_DpeMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "for layer in model.layers[:15]: #freeze up to the last layer\n",
        "    layer.trainable = False\n",
        "model.add(...)  #other layers\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}